{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten()\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.0107e-01, 1.6524e-01, 8.5029e-01, 2.1274e-01, 6.3077e-01,\n",
       "          8.8386e-01, 6.5324e-01, 6.1185e-01, 2.3237e-01, 7.9395e-01,\n",
       "          4.7899e-02, 8.5907e-01, 2.9867e-01, 8.1223e-01, 2.2499e-01,\n",
       "          1.3539e-01, 7.0724e-01, 5.3095e-01, 8.5132e-01, 4.2947e-01,\n",
       "          8.5333e-01, 7.2603e-02, 4.8446e-01, 3.0369e-01, 7.2293e-01,\n",
       "          8.6056e-01, 7.2783e-01, 8.6004e-04],\n",
       "         [8.7178e-01, 2.0585e-01, 3.5467e-01, 6.6485e-01, 6.4393e-01,\n",
       "          2.4276e-02, 5.8983e-01, 2.4818e-01, 7.7257e-01, 3.5956e-01,\n",
       "          7.0484e-01, 5.8190e-01, 6.3531e-01, 6.3390e-01, 7.2670e-01,\n",
       "          9.2514e-01, 7.2127e-01, 8.2472e-01, 4.3066e-01, 7.3061e-01,\n",
       "          9.0026e-01, 5.7986e-01, 4.9032e-01, 7.1654e-01, 6.5162e-01,\n",
       "          5.7642e-01, 9.3393e-01, 9.0961e-01],\n",
       "         [3.9701e-01, 4.1536e-01, 3.4440e-01, 2.4660e-01, 1.1120e-01,\n",
       "          7.5939e-01, 1.7776e-01, 3.5728e-01, 9.3402e-01, 1.8807e-02,\n",
       "          6.7759e-01, 5.9238e-01, 3.3983e-01, 7.9594e-02, 9.0730e-01,\n",
       "          5.2525e-01, 9.2663e-01, 5.2357e-01, 2.3585e-02, 9.5148e-01,\n",
       "          7.3633e-01, 5.2771e-01, 8.7888e-01, 2.4500e-01, 8.6965e-01,\n",
       "          1.3433e-01, 1.4722e-01, 6.7437e-01],\n",
       "         [2.2875e-01, 4.7680e-01, 2.3776e-01, 7.4463e-01, 8.9217e-01,\n",
       "          4.2767e-01, 4.1971e-03, 2.8266e-03, 1.3681e-01, 9.1734e-01,\n",
       "          4.2103e-01, 2.4376e-01, 1.3913e-01, 6.6521e-01, 5.0443e-01,\n",
       "          3.6794e-02, 4.4257e-01, 9.5017e-01, 1.9987e-02, 9.5970e-01,\n",
       "          6.5378e-01, 8.0963e-01, 3.2702e-01, 8.2646e-02, 4.1105e-01,\n",
       "          8.6410e-01, 8.7257e-02, 1.9208e-01],\n",
       "         [1.5131e-01, 5.8835e-01, 5.3230e-01, 7.7357e-02, 8.8842e-01,\n",
       "          2.9210e-01, 5.3300e-01, 7.8216e-02, 3.2248e-01, 9.3968e-01,\n",
       "          1.1157e-01, 6.5017e-01, 4.9202e-01, 3.7910e-01, 9.1925e-01,\n",
       "          3.4853e-01, 7.5614e-01, 2.9468e-02, 6.7192e-01, 8.8602e-01,\n",
       "          2.4872e-01, 6.2814e-01, 1.8053e-01, 5.9855e-01, 6.6330e-01,\n",
       "          6.4478e-01, 9.7873e-01, 2.9863e-01],\n",
       "         [7.0032e-01, 1.9783e-01, 9.3778e-01, 9.8479e-01, 4.1625e-01,\n",
       "          1.4575e-01, 3.6388e-01, 5.2734e-02, 1.7370e-03, 8.2496e-01,\n",
       "          6.5491e-01, 6.5829e-01, 1.1014e-01, 6.3611e-01, 8.0843e-01,\n",
       "          4.2594e-01, 2.0027e-01, 1.5821e-01, 3.0157e-01, 8.2413e-01,\n",
       "          6.1874e-01, 7.7602e-01, 6.9448e-01, 3.3805e-02, 9.4475e-01,\n",
       "          5.9881e-01, 9.9957e-01, 4.4415e-02],\n",
       "         [7.8561e-01, 9.0192e-01, 6.9756e-02, 5.8026e-02, 9.9185e-01,\n",
       "          9.8131e-01, 7.8372e-01, 5.4956e-01, 4.1034e-01, 6.0767e-01,\n",
       "          3.1550e-01, 1.8577e-01, 3.9841e-01, 7.0882e-01, 9.6990e-01,\n",
       "          2.7704e-01, 2.8909e-01, 4.3670e-01, 5.5611e-01, 6.7395e-02,\n",
       "          2.3636e-02, 9.4514e-01, 6.8088e-01, 1.6364e-01, 5.8967e-01,\n",
       "          5.4791e-01, 7.9247e-01, 8.0961e-01],\n",
       "         [8.4399e-01, 5.5606e-02, 6.0658e-01, 4.7832e-01, 6.9619e-01,\n",
       "          2.3930e-01, 2.5716e-01, 4.5670e-01, 4.4977e-01, 3.6934e-01,\n",
       "          6.6433e-02, 5.7596e-01, 4.9564e-01, 8.7995e-01, 5.2359e-01,\n",
       "          4.8880e-01, 6.9571e-01, 2.2791e-01, 9.5850e-01, 8.4785e-01,\n",
       "          2.4949e-01, 4.4092e-01, 8.0828e-01, 6.6906e-02, 4.4677e-01,\n",
       "          2.5815e-01, 8.4294e-01, 9.2536e-01],\n",
       "         [5.5168e-01, 8.6912e-01, 7.3208e-01, 9.9047e-01, 1.7006e-01,\n",
       "          5.2539e-02, 2.1567e-02, 9.7731e-01, 7.4994e-01, 9.7958e-01,\n",
       "          9.3048e-01, 2.8054e-01, 1.5875e-01, 5.7529e-01, 4.2512e-01,\n",
       "          4.3140e-01, 5.6093e-01, 3.0251e-01, 2.8764e-01, 8.7734e-01,\n",
       "          1.8823e-01, 9.5869e-01, 8.5447e-01, 2.8549e-01, 7.5620e-01,\n",
       "          9.4247e-01, 8.2097e-01, 2.1937e-02],\n",
       "         [5.5717e-01, 6.0142e-01, 8.8422e-01, 5.8291e-01, 3.3806e-01,\n",
       "          4.5071e-01, 9.3574e-02, 8.1524e-01, 1.5039e-01, 9.7742e-01,\n",
       "          3.6761e-01, 2.3743e-01, 6.1151e-01, 8.5722e-01, 7.8394e-01,\n",
       "          8.4986e-02, 4.9754e-01, 2.2583e-01, 4.8602e-01, 8.3269e-01,\n",
       "          6.7693e-01, 4.5582e-01, 6.8170e-01, 1.7632e-01, 1.2481e-01,\n",
       "          4.3356e-01, 6.6367e-01, 1.4183e-01],\n",
       "         [9.9758e-01, 1.7513e-01, 5.6480e-01, 9.7844e-01, 6.4584e-01,\n",
       "          4.1922e-01, 7.6935e-01, 2.2053e-01, 5.2810e-01, 1.6764e-02,\n",
       "          7.8614e-01, 8.7003e-01, 8.6780e-02, 7.3197e-03, 5.2968e-01,\n",
       "          4.7097e-01, 4.9799e-01, 9.4291e-02, 4.3402e-01, 9.0961e-01,\n",
       "          8.7219e-01, 8.4106e-01, 3.7810e-01, 2.9150e-02, 4.8597e-01,\n",
       "          7.3550e-01, 4.1523e-01, 4.2903e-01],\n",
       "         [9.5871e-01, 3.6265e-02, 7.1396e-01, 7.5883e-02, 3.5335e-01,\n",
       "          4.1318e-01, 6.7494e-01, 9.1433e-01, 6.8163e-01, 9.3417e-01,\n",
       "          2.8043e-01, 3.3153e-01, 7.2935e-01, 7.3249e-01, 9.6975e-01,\n",
       "          3.5265e-01, 9.6012e-01, 7.9046e-01, 5.1015e-01, 5.2505e-01,\n",
       "          1.7365e-01, 3.5651e-01, 3.6100e-01, 4.6812e-01, 2.5553e-01,\n",
       "          9.6657e-01, 9.8186e-01, 6.4534e-01],\n",
       "         [8.2633e-01, 6.5991e-01, 7.4865e-01, 5.9743e-02, 1.1259e-01,\n",
       "          1.4522e-01, 3.0047e-01, 7.4442e-01, 2.6104e-01, 5.1733e-01,\n",
       "          8.3985e-01, 6.4437e-01, 4.2538e-01, 7.4456e-01, 5.7070e-01,\n",
       "          2.0560e-01, 5.7598e-01, 4.7739e-01, 7.0183e-02, 2.2272e-01,\n",
       "          9.5447e-01, 7.6974e-01, 4.4389e-01, 7.5816e-01, 4.1630e-01,\n",
       "          9.6184e-01, 1.9296e-01, 2.9131e-01],\n",
       "         [6.8920e-01, 8.8625e-01, 8.9277e-01, 7.2046e-01, 1.9661e-01,\n",
       "          6.7903e-01, 5.3942e-01, 4.7444e-01, 2.7339e-01, 7.8624e-01,\n",
       "          5.9226e-01, 8.4713e-01, 4.2020e-01, 4.3002e-01, 3.7157e-01,\n",
       "          5.2826e-01, 7.8757e-02, 2.7959e-01, 3.4725e-01, 6.6134e-01,\n",
       "          2.9807e-02, 9.5583e-01, 5.6416e-01, 9.0746e-01, 2.5776e-01,\n",
       "          9.5118e-01, 3.9042e-01, 7.4296e-01],\n",
       "         [3.8683e-01, 9.6859e-01, 8.9334e-01, 1.5042e-02, 4.1634e-01,\n",
       "          7.9768e-01, 3.6367e-01, 7.8574e-01, 2.0887e-01, 9.2680e-01,\n",
       "          4.2888e-01, 4.8339e-01, 4.8555e-01, 5.2448e-01, 9.4698e-01,\n",
       "          1.0629e-01, 5.9287e-01, 1.7213e-01, 7.5461e-01, 5.7540e-02,\n",
       "          1.5377e-01, 3.3500e-01, 3.6472e-01, 2.7637e-01, 3.5595e-01,\n",
       "          8.8988e-01, 4.4997e-01, 4.1371e-01],\n",
       "         [4.4860e-01, 5.0055e-01, 1.3940e-01, 8.8144e-01, 3.3769e-01,\n",
       "          2.6215e-01, 9.3638e-01, 7.2262e-01, 9.1046e-01, 7.8223e-01,\n",
       "          4.4163e-01, 1.5399e-01, 2.5717e-02, 4.8399e-01, 1.1695e-01,\n",
       "          6.8295e-01, 4.6574e-01, 5.3066e-01, 1.6972e-01, 6.0719e-01,\n",
       "          7.6607e-01, 5.6029e-01, 4.7010e-01, 6.8313e-01, 5.5570e-01,\n",
       "          7.7282e-02, 8.9707e-01, 4.4228e-01],\n",
       "         [8.9371e-01, 8.4127e-01, 3.7966e-01, 1.7856e-01, 4.2149e-01,\n",
       "          9.4932e-01, 1.3366e-01, 2.2222e-02, 1.4420e-01, 1.1289e-01,\n",
       "          8.9450e-01, 3.1125e-01, 5.8153e-01, 7.9142e-01, 3.8654e-01,\n",
       "          9.9146e-01, 2.8917e-01, 7.5869e-01, 7.9110e-01, 8.4956e-02,\n",
       "          3.9341e-01, 9.9003e-01, 8.8469e-01, 5.0031e-01, 6.6997e-01,\n",
       "          7.0457e-01, 2.7894e-01, 4.6658e-01],\n",
       "         [1.5579e-01, 4.8787e-01, 5.2475e-01, 3.4533e-01, 7.3875e-01,\n",
       "          2.7062e-01, 8.2419e-01, 4.4287e-01, 2.6745e-01, 8.3763e-01,\n",
       "          9.7179e-01, 1.1740e-01, 6.7285e-01, 9.4810e-01, 5.5008e-01,\n",
       "          2.9432e-01, 6.7444e-01, 9.5606e-01, 8.2666e-01, 4.1739e-01,\n",
       "          7.2604e-01, 9.2193e-01, 1.6079e-02, 9.3150e-01, 5.4855e-01,\n",
       "          8.0843e-01, 6.7320e-01, 2.4098e-01],\n",
       "         [8.5204e-01, 8.1838e-01, 5.6541e-02, 5.2671e-01, 8.5514e-01,\n",
       "          6.5621e-01, 8.9173e-01, 9.3721e-01, 8.1808e-01, 1.2681e-02,\n",
       "          8.4523e-01, 6.9317e-01, 5.2462e-01, 6.1521e-02, 9.2917e-01,\n",
       "          7.9512e-01, 8.5065e-01, 6.1006e-01, 8.5640e-01, 6.1871e-01,\n",
       "          4.6201e-01, 1.3543e-01, 9.3206e-01, 6.8784e-01, 4.5866e-01,\n",
       "          9.8631e-01, 9.8510e-01, 1.4671e-01],\n",
       "         [4.7924e-02, 8.0790e-01, 1.3224e-01, 6.0236e-01, 5.4271e-01,\n",
       "          6.1189e-01, 8.2411e-01, 6.5283e-01, 5.5037e-01, 8.1052e-01,\n",
       "          1.8820e-02, 2.3308e-01, 6.3978e-01, 9.5963e-01, 9.7938e-01,\n",
       "          2.1361e-01, 4.4614e-01, 2.3922e-01, 3.3701e-01, 6.4707e-01,\n",
       "          8.7376e-01, 1.2284e-01, 7.7046e-04, 4.2757e-03, 9.1595e-01,\n",
       "          1.1628e-01, 5.4427e-01, 1.5795e-01],\n",
       "         [4.2223e-01, 6.2474e-01, 8.3437e-01, 4.3519e-01, 4.0025e-01,\n",
       "          2.2170e-01, 8.8098e-01, 3.3133e-01, 7.3047e-01, 6.2664e-01,\n",
       "          2.9866e-01, 3.2892e-01, 5.1369e-01, 1.7561e-01, 9.7181e-01,\n",
       "          4.9949e-01, 6.6375e-01, 9.5868e-01, 4.2328e-02, 5.9700e-01,\n",
       "          8.6757e-02, 5.7299e-01, 4.8001e-01, 6.1169e-01, 8.5810e-01,\n",
       "          8.2393e-01, 2.3021e-01, 3.1660e-02],\n",
       "         [1.8468e-01, 6.6965e-01, 3.2280e-01, 7.1851e-01, 3.7429e-01,\n",
       "          5.0354e-01, 9.6567e-01, 7.3776e-01, 8.0586e-01, 6.5774e-02,\n",
       "          8.2941e-01, 4.9679e-01, 9.8078e-01, 4.6946e-01, 2.7080e-01,\n",
       "          3.7296e-01, 9.2215e-01, 9.9402e-01, 9.6235e-01, 1.5178e-02,\n",
       "          8.1695e-01, 9.7975e-01, 8.5053e-01, 7.4171e-01, 9.0698e-02,\n",
       "          3.5961e-01, 1.2198e-01, 4.6419e-01],\n",
       "         [7.0534e-01, 9.0114e-01, 7.3644e-01, 7.0709e-01, 9.9040e-01,\n",
       "          5.2672e-01, 4.0930e-01, 9.7578e-02, 5.8021e-02, 5.7614e-01,\n",
       "          4.2848e-01, 3.3405e-02, 7.7405e-01, 7.3094e-01, 7.4413e-01,\n",
       "          1.2791e-01, 1.3227e-01, 5.2519e-01, 7.4703e-01, 2.2787e-01,\n",
       "          2.6045e-02, 4.5492e-01, 5.2136e-01, 8.7756e-01, 2.9232e-01,\n",
       "          8.7650e-01, 7.3064e-01, 1.9177e-01],\n",
       "         [4.6714e-01, 4.5536e-01, 6.8898e-01, 6.6722e-01, 2.8149e-01,\n",
       "          4.0502e-01, 1.3308e-01, 8.6015e-03, 9.9749e-01, 7.5197e-01,\n",
       "          6.8904e-02, 8.5695e-01, 9.7152e-01, 4.9393e-01, 9.7630e-01,\n",
       "          8.2073e-02, 3.3751e-01, 7.7416e-01, 6.9763e-01, 2.5460e-01,\n",
       "          9.5689e-01, 7.7628e-01, 6.3832e-01, 5.8237e-01, 1.4701e-01,\n",
       "          3.4896e-02, 5.7253e-01, 4.5400e-01],\n",
       "         [8.8841e-01, 3.1475e-01, 4.2275e-01, 8.0912e-01, 7.6314e-02,\n",
       "          4.1468e-01, 4.6264e-01, 9.4393e-01, 6.2201e-01, 5.4557e-01,\n",
       "          1.3772e-01, 9.2228e-01, 2.6423e-01, 1.1820e-01, 2.8592e-01,\n",
       "          4.8957e-01, 6.9583e-01, 9.3008e-01, 3.9467e-01, 9.6970e-02,\n",
       "          7.1301e-01, 2.5164e-01, 5.9601e-01, 3.8624e-01, 9.6344e-01,\n",
       "          2.4137e-01, 4.4316e-01, 2.7761e-01],\n",
       "         [8.7328e-01, 5.7887e-01, 7.5437e-01, 7.4420e-01, 6.6662e-01,\n",
       "          1.1079e-01, 6.1449e-01, 4.4747e-01, 2.1945e-02, 9.8197e-01,\n",
       "          6.2190e-01, 3.3127e-01, 3.3519e-01, 1.0918e-01, 2.1808e-01,\n",
       "          7.2828e-01, 3.8368e-01, 8.2148e-01, 4.6423e-01, 6.8538e-01,\n",
       "          9.5773e-01, 9.5502e-01, 9.0104e-01, 3.7194e-01, 2.1495e-01,\n",
       "          8.4444e-01, 5.8155e-01, 9.0832e-01],\n",
       "         [3.6355e-01, 1.0851e-01, 4.2747e-01, 1.6739e-01, 8.5801e-01,\n",
       "          2.8240e-01, 2.4545e-01, 9.3902e-01, 1.5011e-02, 2.5046e-01,\n",
       "          7.3287e-01, 9.8033e-02, 6.9018e-01, 6.2884e-01, 4.4809e-01,\n",
       "          6.1910e-01, 5.8019e-01, 4.9612e-01, 2.9585e-02, 3.6426e-01,\n",
       "          8.0639e-01, 5.3944e-02, 9.7274e-01, 8.0388e-01, 2.1259e-01,\n",
       "          5.6484e-01, 2.1974e-01, 7.4471e-01],\n",
       "         [8.2425e-01, 5.9744e-01, 7.7399e-01, 3.3676e-02, 9.6157e-01,\n",
       "          2.1160e-01, 8.8266e-01, 4.0466e-01, 6.7347e-01, 6.8443e-01,\n",
       "          8.0418e-01, 4.3942e-01, 4.8152e-01, 2.7246e-01, 4.7277e-01,\n",
       "          8.2715e-01, 2.6444e-02, 1.3939e-01, 3.0798e-02, 4.5286e-01,\n",
       "          4.6009e-01, 8.7674e-01, 9.8971e-04, 7.0763e-01, 8.4919e-01,\n",
       "          7.7977e-01, 4.9660e-01, 4.8682e-01]]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0168, -0.0145,  0.0108, -0.0308, -0.0248,  0.1108, -0.0258,  0.0318,\n",
       "         -0.0387, -0.0164]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(X)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1014, 0.0983, 0.1008, 0.0967, 0.0973, 0.1114, 0.0972, 0.1029, 0.0959,\n",
       "         0.0981]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "pred_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([5], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.2282, -0.0438,  0.6194, -0.4067,  0.6907,  0.2030,  0.3688,  0.3396,\n",
      "         -0.6019, -0.4045,  0.1409,  0.2198, -0.0515,  0.0283, -0.3722, -0.1736,\n",
      "          0.1188, -0.2527, -0.3131,  0.0231],\n",
      "        [ 0.0561,  0.4880,  0.6106,  0.1094,  0.6957,  0.4406,  0.2356,  0.2651,\n",
      "         -0.4235, -0.6626,  0.3244,  0.0231,  0.2321, -0.0122, -0.4175,  0.1186,\n",
      "          0.1872, -0.5331, -0.3216,  0.2493],\n",
      "        [-0.0596,  0.3645,  0.6605, -0.0875,  0.2718,  0.0917,  0.0350,  0.2355,\n",
      "         -0.1490, -0.5715,  0.0413,  0.0921,  0.1937,  0.2270, -0.1556, -0.1567,\n",
      "         -0.0662, -0.2231, -0.5896, -0.0056]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.0000, 0.6194, 0.0000, 0.6907, 0.2030, 0.3688, 0.3396, 0.0000,\n",
      "         0.0000, 0.1409, 0.2198, 0.0000, 0.0283, 0.0000, 0.0000, 0.1188, 0.0000,\n",
      "         0.0000, 0.0231],\n",
      "        [0.0561, 0.4880, 0.6106, 0.1094, 0.6957, 0.4406, 0.2356, 0.2651, 0.0000,\n",
      "         0.0000, 0.3244, 0.0231, 0.2321, 0.0000, 0.0000, 0.1186, 0.1872, 0.0000,\n",
      "         0.0000, 0.2493],\n",
      "        [0.0000, 0.3645, 0.6605, 0.0000, 0.2718, 0.0917, 0.0350, 0.2355, 0.0000,\n",
      "         0.0000, 0.0413, 0.0921, 0.1937, 0.2270, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1758,  0.4133,  0.2703,  0.3293,  0.0935, -0.1985, -0.1184,  0.3020,\n",
       "         -0.1306,  0.1350],\n",
       "        [-0.1598,  0.5280,  0.2524,  0.3452,  0.1110, -0.2142, -0.1057,  0.3620,\n",
       "         -0.1912,  0.2405],\n",
       "        [-0.0425,  0.4129,  0.1824,  0.3168,  0.0989, -0.1702, -0.0621,  0.3151,\n",
       "         -0.1726,  0.2237]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0747, 0.1346, 0.1167, 0.1238, 0.0978, 0.0730, 0.0791, 0.1204, 0.0781,\n",
       "         0.1019],\n",
       "        [0.0734, 0.1461, 0.1109, 0.1217, 0.0963, 0.0696, 0.0775, 0.1237, 0.0712,\n",
       "         0.1096],\n",
       "        [0.0841, 0.1327, 0.1053, 0.1205, 0.0969, 0.0740, 0.0825, 0.1203, 0.0739,\n",
       "         0.1098]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "pred_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten()\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0135, -0.0254, -0.0342,  ...,  0.0179,  0.0152, -0.0279],\n",
      "        [-0.0004, -0.0217, -0.0089,  ...,  0.0090, -0.0156, -0.0202]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0269,  0.0312], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0236, -0.0054, -0.0365,  ..., -0.0162,  0.0290, -0.0231],\n",
      "        [ 0.0050, -0.0391,  0.0083,  ...,  0.0302, -0.0248, -0.0208]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0427,  0.0270], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0391,  0.0066,  0.0325,  ..., -0.0100,  0.0179, -0.0135],\n",
      "        [-0.0334, -0.0278, -0.0279,  ..., -0.0027, -0.0235,  0.0425]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0294, 0.0333], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
