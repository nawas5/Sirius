{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a54e6e74",
   "metadata": {},
   "source": [
    "### Установка chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1006b6",
   "metadata": {},
   "source": [
    "1. Скачать и установить google chrome браузер\n",
    "2. Посмотреть версию браузера (настройки с троеточием справа -> Help -> About Google Chrome)\n",
    "3. Скачать и установить google-chrome-driver версии, соответствующей версии браузера https://chromedriver.chromium.org/\n",
    "4. Распаковать архив и положить в <путь к проекту>/bin/chromedriver\n",
    "на виндоуз переименовать чтобы не было .exe в расширении, либо в переменной ниже поменять путь к драйверу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfb92a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q selenium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811cad0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0edf9f4c",
   "metadata": {},
   "source": [
    "### Конфиги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59ac18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME_URL = \"chrome://dino/\"\n",
    "CHROMEDRIVER_PATH = \"./bin/chromedriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb784822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Action(Enum):\n",
    "    DO_NOTHING = 0\n",
    "    JUMP = 1\n",
    "    # TODO: Add action DUSK\n",
    "    #DUSK = 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac768aed",
   "metadata": {},
   "source": [
    "### Среда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "690fd1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import typing\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageGrab\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fcf3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОБЯЗАТЕЛЬНО TODO: Подобрать рамку под себя\n",
    "# Создать environment (см. код ниже)\n",
    "# Начать игру\n",
    "# Подобрать различные варианты через ImageGrab.grab(bbox=(0, 210, 550, 350))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f48fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCREEN_BBOX = (0, 200, 650, 400)\n",
    "SCREEN_BBOX = (100, 150, 350, 450)\n",
    "SCREEN_BBOX = (90, 300, 400, 345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65b5042c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATYAAAAtCAIAAACruNASAAABC0lEQVR4nO3cQQrCMBRFURVXGEoW2UG2WAeBKIiCUs1TzhmEjkomlx8o6XHbtgN80rquY71XSiml9OfW2lhf1V/S1+fvqbWONd959gbg6jYzutPsDQDPmKJMNo6jew3P1tp7R+VMEmW+HSv9sz4PDroQzhQlxf8NwF2YohBNohBNohBNohBNohBNohBNohDNd1E+btwpeXTZ5Ztqrb9yx6UzRfmShDYS9vAqiUK0oyvdkMwUhWgShWgShWgShWgShWgShWinZVlm7wF4yBSFaBKFaBKFaBKFaBKFaBKFaBKFaBKFaBKFaBKFaBKFaBKFaBKFaBKFaBKFaH7SCdFMUYgmUYgmUYgmUYh2Ads3OYlzaAEpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=310x45 at 0x2CE01BF0910>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Запускать для подбора бокса ПОСЛЕ создания энвайронмента ниже\n",
    "ImageGrab.grab(bbox=SCREEN_BBOX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ee8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5944de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63bbed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(\n",
    "        self, \n",
    "        chrome_driver_path: str, \n",
    "        game_url: str, \n",
    "        speed: typing.Optional[int] = None,\n",
    "        acceleration: typing.Optional[float] = None,\n",
    "    ) -> None:\n",
    "        self.driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
    "        # Обязательно TODO: убедиться что эти параметры годятся \n",
    "        self.driver.set_window_position(x=-20, y=-20)\n",
    "        self.driver.set_window_size(550, 450)\n",
    "        try:\n",
    "            self.driver.get('chrome://dino')\n",
    "        except Exception:\n",
    "            pass   # ignore\n",
    "        \n",
    "        if speed:\n",
    "            self.driver.execute_script(f\"Runner.instance_.setSpeed({speed})\")\n",
    "        \n",
    "        if acceleration:\n",
    "            self.driver.execute_script(f\"Runner.config.ACCELERATION={acceleration}\")\n",
    "\n",
    "    def start_game(self):\n",
    "        self.driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.SPACE)\n",
    "        time.sleep(2)\n",
    "        \n",
    "    def close_game(self):\n",
    "        self.driver.close()\n",
    "            \n",
    "    def perform_action(self, action: Action) -> (typing.Optional[np.ndarray], float):\n",
    "        state = self.get_screen()\n",
    "        if action == Action.DO_NOTHING:\n",
    "            pass\n",
    "        elif action == Action.JUMP:\n",
    "            # TODO: Принять решение не нужна ли тут какая-то пауза убедиться что прыжок закончили\n",
    "            self.driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.SPACE)\n",
    "            time.sleep(0.5)\n",
    "        elif action == Action.DUSK:\n",
    "            # TODO: (ТОЛЬКО ЕСЛИ ДОБАВИЛИ СООТВЕТСТВУЮШИЙ ЭКШН В АГЕНТА) \n",
    "            # Если был выбран нырок, то возможно стоит подольше его подержать, а не просто один раз кликнуть\n",
    "            self.driver.driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.ARRAYS_DOWN)\n",
    "        else:\n",
    "            raise Exception(f\"Unsupported action {action}\")\n",
    "        \n",
    "        if self.has_crashed():\n",
    "            return None, 0\n",
    "        else:\n",
    "            # TODO: Попробовать разные варианты вычисления награды. Может штрафовать за лишние прыжки, например\n",
    "            reward = self.get_score() ** 1.1\n",
    "        return state, reward\n",
    "            \n",
    "    def has_crashed(self) -> bool:\n",
    "        return self.driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "    \n",
    "    def restart(self) -> None:\n",
    "        self.driver.execute_script(\"Runner.instance_.restart()\")\n",
    "\n",
    "    def get_screen(self) -> np.ndarray:\n",
    "        screen = ImageGrab.grab(bbox=SCREEN_BBOX)\n",
    "        screen = np.array(screen)\n",
    "        return self.preprocess_screen(screen)\n",
    "        \n",
    "    def preprocess_screen(self, screen: np.ndarray) -> np.ndarray:\n",
    "        # TODO: Добавить больше препроцессинга. Возможные идеи:\n",
    "        #    Убрать облака?\n",
    "        #    Убрать линию \"дороги\"?\n",
    "        #    Обрезать изображение?\n",
    "        #    Сделать черно-белым?\n",
    "        #    Поменять размер?\n",
    "        #    Сделать динозавра прямоугольником? :) \n",
    "        #    Меньше будущей информации?\n",
    "        #    Ваши идеи\n",
    "        return cv2.cvtColor(screen, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "    def get_score(self) -> int:\n",
    "        score_array = self.driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
    "        score = ''.join(score_array)\n",
    "        return int(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8c779cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-1cbb709425cd>:9: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  self.driver = webdriver.Chrome(executable_path=chrome_driver_path)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Подобрать какая скорость больше подойдет\n",
    "# TODO: Принять решение будете ли использовать ускорение или лучше задать его в ноль (тогда не будет птичек еще)\n",
    "\n",
    "environment = Environment(CHROMEDRIVER_PATH, GAME_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "127f00cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.start_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baf25c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[238, 238, 238, ..., 247, 247, 247],\n",
       "        [238, 238, 238, ..., 247, 247, 247],\n",
       "        [238, 238, 238, ..., 247, 247, 247],\n",
       "        ...,\n",
       "        [238, 238, 238, ..., 247, 247, 247],\n",
       "        [238, 238, 238, ..., 247, 247, 247],\n",
       "        [238, 238, 238, ..., 247, 247, 247]], dtype=uint8),\n",
       " 24.03254452572564)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.perform_action(Action.JUMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "151c9795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[238, 238, 238, ..., 247, 247, 247],\n",
       "        [238, 238, 238, ..., 247, 247, 247],\n",
       "        [238, 238, 238, ..., 247, 247, 247],\n",
       "        ...,\n",
       "        [238, 238, 238, ..., 247, 247, 247],\n",
       "        [238, 238, 238, ..., 247, 247, 247],\n",
       "        [238, 238, 238, ..., 247, 247, 247]], dtype=uint8),\n",
       " 25.505213368279996)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.perform_action(Action.DO_NOTHING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636a8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3191b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8083383",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ac319c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, screen_width, screen_height, action_count):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=2)\n",
    "        #conv1_out_width = self.conv2d_size_out(screen_width)\n",
    "        #conv1_out_height = self.conv2d_size_out(screen_height)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        #self.flatten = nn.Flatten()\n",
    "        #self.head = nn.Linear(conv1_out_width * conv1_out_height * 16, action_count)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=5, stride=2) -> int:\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(screen_width)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(screen_height)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, action_count)\n",
    "    def forward(self, x):\n",
    "        # TODO: Добавить различные сверточные слои / функции активации / и т.д.\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6177fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b668e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "907b2b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 310)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screen_height = SCREEN_BBOX[3] - SCREEN_BBOX[1]\n",
    "screen_width = SCREEN_BBOX[2] - SCREEN_BBOX[0]\n",
    "\n",
    "screen_height, screen_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8448307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(screen_height, screen_width, len(Action)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2e5af0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 45, 310])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.from_numpy(environment.get_screen()).float()\n",
    "batch_state = state.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "batch_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe452b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5375, 0.7409]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net(batch_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f465cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3793f721",
   "metadata": {},
   "source": [
    "### Агент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd71a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import collections as col\n",
    "\n",
    "Transition = col.namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = col.deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    # TODO: ВАЖНО: Подобрать параметры ниже    \n",
    "    GAMMA = 0.999\n",
    "\n",
    "    EXPLORATION_STEPS_COUNT = 30   # Количество шагов, когда совершаем ТОЛЬКО exploration \n",
    "    EPS_INITIAL = 0.9   # Начальное значение EPS, после которого начинаем подключать exploitation\n",
    "    EPS_FINAL = 0.05   # Минимальное значение EPS, до которого дойдем\n",
    "    EPS_DECAY_DELTA = 0.05   # Величина, на которую будем уменьшать EPS\n",
    "    EPS_DECAY_STEPS_COUNT = 15  # Через сколько шагов будем снижать EPS\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.eps = 1    # По началу только exploration\n",
    "        self.made_steps_count = 0\n",
    "        self.n_actions = len(Action)\n",
    "        self.policy_net = DQN(screen_height, screen_width, self.n_actions).to(device)\n",
    "        self.optimizer = torch.optim.RMSprop(policy_net.parameters())\n",
    "        self.memory = ReplayMemory(10000)\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> Action:\n",
    "        self.made_steps_count += 1\n",
    "        \n",
    "        # Пока еще только exploration\n",
    "        if self.made_steps_count < self.EXPLORATION_STEPS_COUNT:\n",
    "            self.eps = 1\n",
    "        # Начинаем подключать exploitation\n",
    "        elif self.made_steps_count == self.EXPLORATION_STEPS_COUNT:\n",
    "            self.eps = self.EPS_INITIAL\n",
    "        # Каждые EPS_DECAY_STEPS_COUNT шагов на константу уменьшаем eps если он получится не меньше чем EPS_FINAL\n",
    "        elif self.made_steps_count % self.EPS_DECAY_STEPS_COUNT == 0 \\\n",
    "                and self.eps - self.EPS_DECAY_DELTA >= self.EPS_FINAL:\n",
    "            self.eps -= self.EPS_DECAY_DELTA                        \n",
    "        \n",
    "        exploit_prob = random.random()\n",
    "        if exploit_prob > self.eps:\n",
    "            with torch.no_grad():\n",
    "                # Сеть ожидает что у нас обязательно будет размерность для батча. Создаем фиктивную размерность \n",
    "                # Также создаем фиктивную размерность для числа каналов\n",
    "                state_batch = torch.from_numpy(state).float().unsqueeze(0).unsqueeze(0)\n",
    "                batch_action_rewards = policy_net(state_batch)\n",
    "                batch_argmax_actions = batch_action_rewards.max(1).indices\n",
    "                # Взяли нулевой элемент, т.к. у нас батч всего из одного элемента\n",
    "                action_ind = batch_argmax_actions.tolist()[0]\n",
    "        else:\n",
    "            action_ind = random.randrange(self.n_actions)        \n",
    "        return Action(action_ind)    \n",
    "    \n",
    "    def update_reward(self, state: np.ndarray, action: Action, next_state: np.ndarray, reward: int) -> None:\n",
    "        state_batch = torch.from_numpy(state).float().unsqueeze(0).unsqueeze(0)\n",
    "        next_state_batch = torch.from_numpy(next_state).float().unsqueeze(0).unsqueeze(0)\n",
    "        # Индексы действий что мы проделали. Используется чтобы достать текушие оценки Q для проделанных действий\n",
    "        action_batch = torch.from_numpy(np.array([action.value])).unsqueeze(0)\n",
    "        #reward_batch = torch.from_numpy(state).float().unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Получаем текущее \"жадное\" действие для уравнения Беллмана\n",
    "        with torch.no_grad():\n",
    "            next_state_values = policy_net(next_state_batch).max(1)[0]\n",
    "            expected_state_action_values = (next_state_values * GAMMA) + reward\n",
    "\n",
    "        # Получаем текущее значение Q для проделанного действия    \n",
    "        state_action_values = policy_net(state_batch).gather(1, action_batch.type(torch.int64))\n",
    "\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72b0a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ef3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f6da413",
   "metadata": {},
   "source": [
    "### Цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce00d694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running episode 0\n",
      "[[255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " ...\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]] Action.DO_NOTHING\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GAMMA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-8d9d721460f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m#agent.memory.push(state, action, next_state, reward)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-9f45eaa4b16d>\u001b[0m in \u001b[0;36mupdate_reward\u001b[1;34m(self, state, action, next_state, reward)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0mnext_state_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mexpected_state_action_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnext_state_values\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# Получаем текущее значение Q для проделанного действия\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GAMMA' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: число эпизодов подобрать\n",
    "episodes_count = 200\n",
    "\n",
    "\n",
    "\n",
    "for episode_num in range(episodes_count):\n",
    "    print(f\"Running episode {episode_num}\")\n",
    "    environment.restart()\n",
    "    environment.start_game()\n",
    "    \n",
    "    while not environment.has_crashed():\n",
    "        state = environment.get_screen()\n",
    "        if not environment.has_crashed(): \n",
    "            action = agent.select_action(state)\n",
    "            print(state, action)\n",
    "            next_state, reward = environment.perform_action(action)\n",
    "            # Не сломались пока выполняли действие\n",
    "            #agent.memory.push(state, action, next_state, reward)\n",
    "            if next_state is not None:\n",
    "                agent.update_reward(state, action, next_state, reward)\n",
    "                \n",
    "            \n",
    "# TODO: Если так ничего хорошего не выйдет, то делать Replay Memory\n",
    "#https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36411469",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144eddd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
